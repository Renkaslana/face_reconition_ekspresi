{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎭 EMOTION DETECTION - OPTIMIZED FOR REAL-TIME\n",
    "\n",
    "## 🎯 Objectives:\n",
    "- **Balanced Performance**: Akurasi seimbang untuk semua kelas (>65%)\n",
    "- **Real-time Ready**: FPS tinggi untuk webcam detection (>20 FPS)\n",
    "- **Production Quality**: Stable dan reliable\n",
    "\n",
    "## 📊 Dataset:\n",
    "- Training: 20,573 images (5 kelas)\n",
    "- Test: 5,114 images\n",
    "- Format: 48x48 grayscale\n",
    "\n",
    "## 🚀 Strategy:\n",
    "1. ✅ Focal Loss untuk class imbalance\n",
    "2. ✅ Enhanced Class Weights\n",
    "3. ✅ Advanced Augmentation\n",
    "4. ✅ Deep CNN Architecture\n",
    "5. ✅ Real-time Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "🎭 EMOTION DETECTION - OPTIMIZED FOR REAL-TIME\n",
      "======================================================================\n",
      "✅ TensorFlow: 2.20.0\n",
      "✅ Setup Complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup & Imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🎭 EMOTION DETECTION - OPTIMIZED FOR REAL-TIME\")\n",
    "print(\"=\"*70)\n",
    "print(f\"✅ TensorFlow: {tf.__version__}\")\n",
    "print(f\"✅ Setup Complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Focal Loss Implementation Ready\n",
      "   • Gamma: 2.0 (focusing parameter)\n",
      "   • Alpha: 0.25 (balancing factor)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Focal Loss Implementation\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Focal Loss untuk mengatasi class imbalance\n",
    "    Fokus pada hard examples dan minoritas kelas\n",
    "    \"\"\"\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        \n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "        pt = K.sum(y_true * y_pred, axis=-1, keepdims=True)\n",
    "        focal_weight = K.pow(1.0 - pt, gamma)\n",
    "        alpha_weight = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "        loss = focal_weight * alpha_weight * cross_entropy\n",
    "        \n",
    "        return K.mean(K.sum(loss, axis=-1))\n",
    "    \n",
    "    return focal_loss_fixed\n",
    "\n",
    "print(\"✅ Focal Loss Implementation Ready\")\n",
    "print(\"   • Gamma: 2.0 (focusing parameter)\")\n",
    "print(\"   • Alpha: 0.25 (balancing factor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚖️  ENHANCED CLASS WEIGHTS:\n",
      "   MARAH     : weight= 0.36\n",
      "   JIJIK     : weight=10.00\n",
      "   TAKUT     : weight= 0.35\n",
      "   SENANG    : weight= 0.15\n",
      "   SEDIH     : weight= 0.27\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Enhanced Class Weights Calculator\n",
    "\n",
    "def calculate_enhanced_class_weights(train_path=\"train\", power=1.5):\n",
    "    \"\"\"\n",
    "    Enhanced class weights dengan exponential scaling\n",
    "    \"\"\"\n",
    "    emotion_mapping = {\n",
    "        'angry': 0, 'disgusted': 1, 'fearful': 2,\n",
    "        'happy': 3, 'sad': 4\n",
    "    }\n",
    "    \n",
    "    class_counts = {}\n",
    "    total_samples = 0\n",
    "    \n",
    "    for class_name, class_idx in emotion_mapping.items():\n",
    "        class_path = Path(train_path) / class_name\n",
    "        if class_path.exists():\n",
    "            count = len(list(class_path.glob(\"*.png\")))\n",
    "            class_counts[class_idx] = count\n",
    "            total_samples += count\n",
    "    \n",
    "    num_classes = len(class_counts)\n",
    "    class_weights = {}\n",
    "    \n",
    "    for class_idx, count in class_counts.items():\n",
    "        base_weight = total_samples / (num_classes * count)\n",
    "        enhanced_weight = base_weight ** power\n",
    "        class_weights[class_idx] = enhanced_weight\n",
    "    \n",
    "    # Normalize\n",
    "    max_weight = max(class_weights.values())\n",
    "    class_weights = {k: (v/max_weight) * 10 for k, v in class_weights.items()}\n",
    "    \n",
    "    print(\"\\n⚖️  ENHANCED CLASS WEIGHTS:\")\n",
    "    class_names = ['MARAH', 'JIJIK', 'TAKUT', 'SENANG', 'SEDIH']\n",
    "    for idx, name in enumerate(class_names):\n",
    "        print(f\"   {name:<10}: weight={class_weights[idx]:>5.2f}\")\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "enhanced_weights = calculate_enhanced_class_weights(power=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20573 images belonging to 5 classes.\n",
      "Found 5114 images belonging to 5 classes.\n",
      "\n",
      "📊 Data Generators Created:\n",
      "   Train: 20,573 images\n",
      "   Val: 5,114 images\n",
      "   Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Advanced Data Augmentation\n",
    "\n",
    "def create_data_generators(batch_size=32, img_size=(48, 48)):\n",
    "    \"\"\"\n",
    "    Advanced augmentation untuk training\n",
    "    \"\"\"\n",
    "    emotion_classes = ['angry', 'disgusted', 'fearful', 'happy', 'sad']\n",
    "    \n",
    "    # Training augmentation - AGRESIF\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=25,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        brightness_range=[0.8, 1.2]\n",
    "    )\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "        'train',\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical',\n",
    "        classes=emotion_classes,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_gen = val_datagen.flow_from_directory(\n",
    "        'test',\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical',\n",
    "        classes=emotion_classes,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📊 Data Generators Created:\")\n",
    "    print(f\"   Train: {train_gen.samples:,} images\")\n",
    "    print(f\"   Val: {val_gen.samples:,} images\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    \n",
    "    return train_gen, val_gen\n",
    "\n",
    "train_gen, val_gen = create_data_generators(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Creating Optimized Emotion Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Optimized_Emotion_CNN\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Optimized_Emotion_CNN\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_5                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │         \u001b[38;5;34m147,584\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m295,168\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │         \u001b[38;5;34m590,080\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_5                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │           \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m262,656\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │           \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │           \u001b[38;5;34m1,285\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,728,389</span> (10.41 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,728,389\u001b[0m (10.41 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,724,037</span> (10.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,724,037\u001b[0m (10.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,352</span> (17.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,352\u001b[0m (17.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Model Created:\n",
      "   Parameters: 2,728,389\n",
      "   Size: ~10.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Optimized CNN Model\n",
    "\n",
    "def create_optimized_emotion_model(num_classes=5, input_shape=(48, 48, 1)):\n",
    "    \"\"\"\n",
    "    Deep CNN optimized untuk emotion detection real-time\n",
    "    \"\"\"\n",
    "    print(\"\\n🧠 Creating Optimized Emotion Model...\")\n",
    "    \n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        \n",
    "        # Block 1\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 2\n",
    "        Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 3\n",
    "        Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 4\n",
    "        Conv2D(512, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # Classification\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ], name='Optimized_Emotion_CNN')\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    print(f\"\\n✅ Model Created:\")\n",
    "    print(f\"   Parameters: {model.count_params():,}\")\n",
    "    print(f\"   Size: ~{model.count_params() * 4 / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_optimized_emotion_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Training...\n",
      "\n",
      "Epoch 1/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 638ms/step - accuracy: 0.2545 - loss: 0.1791\n",
      "Epoch 1: val_accuracy improved from None to 0.31952, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 698ms/step - accuracy: 0.2709 - loss: 0.1503 - val_accuracy: 0.3195 - val_loss: 0.2222 - learning_rate: 0.0010\n",
      "Epoch 2/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636ms/step - accuracy: 0.3010 - loss: 0.1149\n",
      "Epoch 2: val_accuracy improved from 0.31952 to 0.34904, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 676ms/step - accuracy: 0.3131 - loss: 0.1107 - val_accuracy: 0.3490 - val_loss: 0.2101 - learning_rate: 0.0010\n",
      "Epoch 3/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628ms/step - accuracy: 0.3249 - loss: 0.1065\n",
      "Epoch 3: val_accuracy improved from 0.34904 to 0.35706, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 668ms/step - accuracy: 0.3303 - loss: 0.1061 - val_accuracy: 0.3571 - val_loss: 0.2086 - learning_rate: 0.0010\n",
      "Epoch 4/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656ms/step - accuracy: 0.3260 - loss: 0.1095\n",
      "Epoch 4: val_accuracy did not improve from 0.35706\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 701ms/step - accuracy: 0.3293 - loss: 0.1056 - val_accuracy: 0.3447 - val_loss: 0.2112 - learning_rate: 0.0010\n",
      "Epoch 5/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682ms/step - accuracy: 0.3426 - loss: 0.1050\n",
      "Epoch 5: val_accuracy did not improve from 0.35706\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 746ms/step - accuracy: 0.3416 - loss: 0.1042 - val_accuracy: 0.3050 - val_loss: 0.2220 - learning_rate: 0.0010\n",
      "Epoch 6/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723ms/step - accuracy: 0.3397 - loss: 0.1055\n",
      "Epoch 6: val_accuracy did not improve from 0.35706\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m518s\u001b[0m 770ms/step - accuracy: 0.3399 - loss: 0.1042 - val_accuracy: 0.2039 - val_loss: 0.2293 - learning_rate: 0.0010\n",
      "Epoch 7/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683ms/step - accuracy: 0.3495 - loss: 0.0996\n",
      "Epoch 7: val_accuracy did not improve from 0.35706\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 729ms/step - accuracy: 0.3422 - loss: 0.1033 - val_accuracy: 0.2356 - val_loss: 0.2561 - learning_rate: 0.0010\n",
      "Epoch 8/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 732ms/step - accuracy: 0.3465 - loss: 0.1075\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.35706\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 778ms/step - accuracy: 0.3591 - loss: 0.1025 - val_accuracy: 0.3072 - val_loss: 0.2260 - learning_rate: 0.0010\n",
      "Epoch 9/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785ms/step - accuracy: 0.3891 - loss: 0.0959\n",
      "Epoch 9: val_accuracy improved from 0.35706 to 0.45405, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m591s\u001b[0m 915ms/step - accuracy: 0.3988 - loss: 0.0968 - val_accuracy: 0.4540 - val_loss: 0.1867 - learning_rate: 5.0000e-04\n",
      "Epoch 10/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671ms/step - accuracy: 0.4264 - loss: 0.0939\n",
      "Epoch 10: val_accuracy improved from 0.45405 to 0.49198, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 710ms/step - accuracy: 0.4377 - loss: 0.0908 - val_accuracy: 0.4920 - val_loss: 0.1690 - learning_rate: 5.0000e-04\n",
      "Epoch 11/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632ms/step - accuracy: 0.4621 - loss: 0.0865\n",
      "Epoch 11: val_accuracy improved from 0.49198 to 0.49707, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m432s\u001b[0m 672ms/step - accuracy: 0.4749 - loss: 0.0850 - val_accuracy: 0.4971 - val_loss: 0.1746 - learning_rate: 5.0000e-04\n",
      "Epoch 12/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 630ms/step - accuracy: 0.5142 - loss: 0.0828\n",
      "Epoch 12: val_accuracy improved from 0.49707 to 0.54185, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m430s\u001b[0m 669ms/step - accuracy: 0.5156 - loss: 0.0796 - val_accuracy: 0.5418 - val_loss: 0.1486 - learning_rate: 5.0000e-04\n",
      "Epoch 13/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632ms/step - accuracy: 0.5379 - loss: 0.0757\n",
      "Epoch 13: val_accuracy improved from 0.54185 to 0.57098, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 675ms/step - accuracy: 0.5359 - loss: 0.0760 - val_accuracy: 0.5710 - val_loss: 0.1427 - learning_rate: 5.0000e-04\n",
      "Epoch 14/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661ms/step - accuracy: 0.5512 - loss: 0.0715\n",
      "Epoch 14: val_accuracy improved from 0.57098 to 0.59327, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m458s\u001b[0m 700ms/step - accuracy: 0.5497 - loss: 0.0730 - val_accuracy: 0.5933 - val_loss: 0.1339 - learning_rate: 5.0000e-04\n",
      "Epoch 15/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646ms/step - accuracy: 0.5683 - loss: 0.0683\n",
      "Epoch 15: val_accuracy improved from 0.59327 to 0.59777, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 686ms/step - accuracy: 0.5642 - loss: 0.0701 - val_accuracy: 0.5978 - val_loss: 0.1361 - learning_rate: 5.0000e-04\n",
      "Epoch 16/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641ms/step - accuracy: 0.5777 - loss: 0.0675\n",
      "Epoch 16: val_accuracy did not improve from 0.59777\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 683ms/step - accuracy: 0.5744 - loss: 0.0675 - val_accuracy: 0.5894 - val_loss: 0.1305 - learning_rate: 5.0000e-04\n",
      "Epoch 17/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675ms/step - accuracy: 0.5863 - loss: 0.0691\n",
      "Epoch 17: val_accuracy improved from 0.59777 to 0.62104, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 717ms/step - accuracy: 0.5838 - loss: 0.0651 - val_accuracy: 0.6210 - val_loss: 0.1206 - learning_rate: 5.0000e-04\n",
      "Epoch 18/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 626ms/step - accuracy: 0.5871 - loss: 0.0665\n",
      "Epoch 18: val_accuracy improved from 0.62104 to 0.64803, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 665ms/step - accuracy: 0.5925 - loss: 0.0639 - val_accuracy: 0.6480 - val_loss: 0.1128 - learning_rate: 5.0000e-04\n",
      "Epoch 19/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 622ms/step - accuracy: 0.6032 - loss: 0.0625\n",
      "Epoch 19: val_accuracy did not improve from 0.64803\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 660ms/step - accuracy: 0.5994 - loss: 0.0628 - val_accuracy: 0.6375 - val_loss: 0.1129 - learning_rate: 5.0000e-04\n",
      "Epoch 20/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642ms/step - accuracy: 0.6059 - loss: 0.0628\n",
      "Epoch 20: val_accuracy did not improve from 0.64803\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m439s\u001b[0m 682ms/step - accuracy: 0.6046 - loss: 0.0615 - val_accuracy: 0.6326 - val_loss: 0.1137 - learning_rate: 5.0000e-04\n",
      "Epoch 21/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6067 - loss: 0.0589\n",
      "Epoch 21: val_accuracy did not improve from 0.64803\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1552s\u001b[0m 2s/step - accuracy: 0.6104 - loss: 0.0600 - val_accuracy: 0.6355 - val_loss: 0.1160 - learning_rate: 5.0000e-04\n",
      "Epoch 22/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 604ms/step - accuracy: 0.6157 - loss: 0.0597\n",
      "Epoch 22: val_accuracy improved from 0.64803 to 0.65565, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 640ms/step - accuracy: 0.6180 - loss: 0.0592 - val_accuracy: 0.6557 - val_loss: 0.1081 - learning_rate: 5.0000e-04\n",
      "Epoch 23/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 602ms/step - accuracy: 0.6207 - loss: 0.0574\n",
      "Epoch 23: val_accuracy did not improve from 0.65565\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 638ms/step - accuracy: 0.6166 - loss: 0.0579 - val_accuracy: 0.6144 - val_loss: 0.1224 - learning_rate: 5.0000e-04\n",
      "Epoch 24/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 619ms/step - accuracy: 0.6186 - loss: 0.0578\n",
      "Epoch 24: val_accuracy did not improve from 0.65565\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 654ms/step - accuracy: 0.6216 - loss: 0.0573 - val_accuracy: 0.6412 - val_loss: 0.1081 - learning_rate: 5.0000e-04\n",
      "Epoch 25/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599ms/step - accuracy: 0.6277 - loss: 0.0557\n",
      "Epoch 25: val_accuracy improved from 0.65565 to 0.65604, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 636ms/step - accuracy: 0.6268 - loss: 0.0566 - val_accuracy: 0.6560 - val_loss: 0.1102 - learning_rate: 5.0000e-04\n",
      "Epoch 26/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597ms/step - accuracy: 0.6233 - loss: 0.0575\n",
      "Epoch 26: val_accuracy did not improve from 0.65604\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 633ms/step - accuracy: 0.6280 - loss: 0.0559 - val_accuracy: 0.6537 - val_loss: 0.1079 - learning_rate: 5.0000e-04\n",
      "Epoch 27/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 593ms/step - accuracy: 0.6413 - loss: 0.0567\n",
      "Epoch 27: val_accuracy improved from 0.65604 to 0.67032, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 630ms/step - accuracy: 0.6339 - loss: 0.0557 - val_accuracy: 0.6703 - val_loss: 0.1010 - learning_rate: 5.0000e-04\n",
      "Epoch 28/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594ms/step - accuracy: 0.6426 - loss: 0.0545\n",
      "Epoch 28: val_accuracy did not improve from 0.67032\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 630ms/step - accuracy: 0.6424 - loss: 0.0541 - val_accuracy: 0.6641 - val_loss: 0.1054 - learning_rate: 5.0000e-04\n",
      "Epoch 29/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 594ms/step - accuracy: 0.6461 - loss: 0.0535\n",
      "Epoch 29: val_accuracy did not improve from 0.67032\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 630ms/step - accuracy: 0.6465 - loss: 0.0536 - val_accuracy: 0.6508 - val_loss: 0.1068 - learning_rate: 5.0000e-04\n",
      "Epoch 30/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601ms/step - accuracy: 0.6431 - loss: 0.0521\n",
      "Epoch 30: val_accuracy did not improve from 0.67032\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 637ms/step - accuracy: 0.6430 - loss: 0.0537 - val_accuracy: 0.6650 - val_loss: 0.0993 - learning_rate: 5.0000e-04\n",
      "Epoch 31/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 595ms/step - accuracy: 0.6483 - loss: 0.0515\n",
      "Epoch 31: val_accuracy improved from 0.67032 to 0.68557, saving model to best_emotion_model.keras\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 631ms/step - accuracy: 0.6496 - loss: 0.0523 - val_accuracy: 0.6856 - val_loss: 0.0978 - learning_rate: 5.0000e-04\n",
      "Epoch 32/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 596ms/step - accuracy: 0.6485 - loss: 0.0528\n",
      "Epoch 32: val_accuracy did not improve from 0.68557\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 633ms/step - accuracy: 0.6451 - loss: 0.0528 - val_accuracy: 0.6713 - val_loss: 0.1004 - learning_rate: 5.0000e-04\n",
      "Epoch 33/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 597ms/step - accuracy: 0.6540 - loss: 0.0493\n",
      "Epoch 33: val_accuracy did not improve from 0.68557\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m407s\u001b[0m 633ms/step - accuracy: 0.6493 - loss: 0.0518 - val_accuracy: 0.6470 - val_loss: 0.1069 - learning_rate: 5.0000e-04\n",
      "Epoch 34/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 599ms/step - accuracy: 0.6541 - loss: 0.0530\n",
      "Epoch 34: val_accuracy did not improve from 0.68557\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m409s\u001b[0m 635ms/step - accuracy: 0.6561 - loss: 0.0515 - val_accuracy: 0.6854 - val_loss: 0.0968 - learning_rate: 5.0000e-04\n",
      "Epoch 35/35\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601ms/step - accuracy: 0.6535 - loss: 0.0522\n",
      "Epoch 35: val_accuracy did not improve from 0.68557\n",
      "\u001b[1m643/643\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 637ms/step - accuracy: 0.6545 - loss: 0.0512 - val_accuracy: 0.6842 - val_loss: 0.0948 - learning_rate: 5.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "\n",
      "✅ Training Complete!\n",
      "   Time: 275.6 min\n",
      "   Best Accuracy: 0.6856 (68.56%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Training with Callbacks\n",
    "\n",
    "print(\"🚀 Starting Training...\\n\")\n",
    "\n",
    "log_dir = f\"logs/optimized_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=12,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_emotion_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    TensorBoard(log_dir=log_dir)\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=35,\n",
    "    validation_data=val_gen,\n",
    "    class_weight=enhanced_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "best_acc = max(history.history['val_accuracy'])\n",
    "\n",
    "print(f\"\\n✅ Training Complete!\")\n",
    "print(f\"   Time: {training_time/60:.1f} min\")\n",
    "print(f\"   Best Accuracy: {best_acc:.4f} ({best_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 7: Visualize Training Results\u001b[39;00m\n\u001b[32m      3\u001b[39m fig, (ax1, ax2) = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m epochs_range = \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mhistory\u001b[49m.history[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]) + \u001b[32m1\u001b[39m)\n\u001b[32m      7\u001b[39m ax1.plot(epochs_range, history.history[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mb-\u001b[39m\u001b[33m'\u001b[39m, label=\u001b[33m'\u001b[39m\u001b[33mTrain\u001b[39m\u001b[33m'\u001b[39m, lw=\u001b[32m2\u001b[39m)\n\u001b[32m      8\u001b[39m ax1.plot(epochs_range, history.history[\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m'\u001b[39m\u001b[33mr-\u001b[39m\u001b[33m'\u001b[39m, label=\u001b[33m'\u001b[39m\u001b[33mVal\u001b[39m\u001b[33m'\u001b[39m, lw=\u001b[32m2\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 7: Visualize Training Results\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "epochs_range = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "ax1.plot(epochs_range, history.history['accuracy'], 'b-', label='Train', lw=2)\n",
    "ax1.plot(epochs_range, history.history['val_accuracy'], 'r-', label='Val', lw=2)\n",
    "ax1.set_title('Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(epochs_range, history.history['loss'], 'b-', label='Train', lw=2)\n",
    "ax2.plot(epochs_range, history.history['val_loss'], 'r-', label='Val', lw=2)\n",
    "ax2.set_title('Loss', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best Val Accuracy: {max(history.history['val_accuracy']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Evaluating Model...\n",
      "\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 146ms/step\n",
      "\n",
      "📊 CLASSIFICATION REPORT:\n",
      "============================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       MARAH     0.5884    0.5699    0.5790       958\n",
      "       JIJIK     0.4923    0.5766    0.5311       111\n",
      "       TAKUT     0.6133    0.3779    0.4677      1024\n",
      "      SENANG     0.8858    0.9047    0.8951      1774\n",
      "       SEDIH     0.5604    0.7249    0.6322      1247\n",
      "\n",
      "    accuracy                         0.6856      5114\n",
      "   macro avg     0.6280    0.6308    0.6210      5114\n",
      "weighted avg     0.6876    0.6856    0.6783      5114\n",
      "\n",
      "\n",
      "📊 PER-CLASS ACCURACY:\n",
      "------------------------------------------------------------\n",
      "MARAH     : 0.5699 ( 57.0%) ██████████████████████ ⚠️\n",
      "JIJIK     : 0.5766 ( 57.7%) ███████████████████████ ⚠️\n",
      "TAKUT     : 0.3779 ( 37.8%) ███████████████ ❌\n",
      "SENANG    : 0.9047 ( 90.5%) ████████████████████████████████████ ✅\n",
      "SEDIH     : 0.7249 ( 72.5%) ████████████████████████████ ✅\n",
      "\n",
      "⚖️  Balance Score: 0.418\n",
      "📊 Average Accuracy: 0.6308\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Comprehensive Evaluation\n",
    "\n",
    "best_model = keras.models.load_model(\n",
    "    'best_emotion_model.keras',\n",
    "    custom_objects={'focal_loss_fixed': focal_loss(2.0, 0.25)}\n",
    ")\n",
    "\n",
    "print(\"📈 Evaluating Model...\\n\")\n",
    "\n",
    "val_gen.reset()\n",
    "predictions = best_model.predict(val_gen, verbose=1)\n",
    "y_pred = np.argmax(predictions, axis=1)\n",
    "y_true = val_gen.classes\n",
    "\n",
    "class_names = ['MARAH', 'JIJIK', 'TAKUT', 'SENANG', 'SEDIH']\n",
    "\n",
    "print(\"\\n📊 CLASSIFICATION REPORT:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"\\n📊 PER-CLASS ACCURACY:\")\n",
    "print(\"-\"*60)\n",
    "accuracies = []\n",
    "for i, name in enumerate(class_names):\n",
    "    acc = cm[i, i] / cm[i].sum()\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    status = \"✅\" if acc >= 0.65 else \"⚠️\" if acc >= 0.55 else \"❌\"\n",
    "    bar = \"█\" * int(acc * 40)\n",
    "    print(f\"{name:<10}: {acc:.4f} ({acc*100:>5.1f}%) {bar} {status}\")\n",
    "\n",
    "balance_score = min(accuracies) / max(accuracies)\n",
    "print(f\"\\n⚖️  Balance Score: {balance_score:.3f}\")\n",
    "print(f\"📊 Average Accuracy: {np.mean(accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 9: Confusion Matrix Visualization\u001b[39;00m\n\u001b[32m      3\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m8\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m cm_norm = \u001b[43mcm\u001b[49m.astype(\u001b[33m'\u001b[39m\u001b[33mfloat\u001b[39m\u001b[33m'\u001b[39m) / cm.sum(axis=\u001b[32m1\u001b[39m)[:, np.newaxis]\n\u001b[32m      7\u001b[39m sns.heatmap(cm_norm, annot=\u001b[38;5;28;01mTrue\u001b[39;00m, fmt=\u001b[33m'\u001b[39m\u001b[33m.2\u001b[39m\u001b[33m%\u001b[39m\u001b[33m'\u001b[39m, cmap=\u001b[33m'\u001b[39m\u001b[33mBlues\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      8\u001b[39m             xticklabels=class_names, yticklabels=class_names,\n\u001b[32m      9\u001b[39m             cbar_kws={\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mPercentage\u001b[39m\u001b[33m'\u001b[39m},\n\u001b[32m     10\u001b[39m             linewidths=\u001b[32m1\u001b[39m, linecolor=\u001b[33m'\u001b[39m\u001b[33mgray\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mConfusion Matrix\u001b[39m\u001b[33m'\u001b[39m, fontsize=\u001b[32m16\u001b[39m, fontweight=\u001b[33m'\u001b[39m\u001b[33mbold\u001b[39m\u001b[33m'\u001b[39m, pad=\u001b[32m20\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'cm' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 9: Confusion Matrix Visualization\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Percentage'},\n",
    "            linewidths=1, linecolor='gray')\n",
    "\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Confusion matrix saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📹 Real-time Detection Started\n",
      "   Press 'Q' to quit\n",
      "   Press 'S' to save screenshot\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Real-time Webcam Detection\n",
    "\n",
    "def realtime_emotion_detection():\n",
    "    \"\"\"\n",
    "    Real-time emotion detection dengan webcam\n",
    "    Optimized untuk FPS tinggi\n",
    "    \"\"\"\n",
    "    model = keras.models.load_model(\n",
    "        'best_emotion_model.keras',\n",
    "        custom_objects={'focal_loss_fixed': focal_loss(2.0, 0.25)}\n",
    "    )\n",
    "    \n",
    "    face_cascade = cv2.CascadeClassifier(\n",
    "        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "    )\n",
    "    \n",
    "    emotions = ['MARAH', 'JIJIK', 'TAKUT', 'SENANG', 'SEDIH']\n",
    "    colors = [(0,0,255), (0,255,255), (128,0,128), (0,255,0), (255,0,0)]\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "    \n",
    "    print(\"\\n📹 Real-time Detection Started\")\n",
    "    print(\"   Press 'Q' to quit\")\n",
    "    print(\"   Press 'S' to save screenshot\\n\")\n",
    "    \n",
    "    fps_time = time.time()\n",
    "    fps = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray, scaleFactor=1.1, minNeighbors=5,\n",
    "            minSize=(48, 48), maxSize=(300, 300)\n",
    "        )\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            face = cv2.resize(gray[y:y+h, x:x+w], (48, 48))\n",
    "            face_input = face.reshape(1, 48, 48, 1) / 255.0\n",
    "            \n",
    "            pred = model.predict(face_input, verbose=0)\n",
    "            idx = np.argmax(pred)\n",
    "            emotion = emotions[idx]\n",
    "            confidence = pred[0][idx] * 100\n",
    "            \n",
    "            # Draw rectangle dan text\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), colors[idx], 3)\n",
    "            \n",
    "            # Background untuk text\n",
    "            text = f'{emotion}: {confidence:.1f}%'\n",
    "            (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "            cv2.rectangle(frame, (x, y-th-10), (x+tw+10, y), colors[idx], -1)\n",
    "            cv2.putText(frame, text, (x+5, y-5),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n",
    "        \n",
    "        # FPS counter\n",
    "        if time.time() - fps_time > 1:\n",
    "            fps_time = time.time()\n",
    "            fps = fps * 0.7 + 0.3 / (time.time() - fps_time)\n",
    "        \n",
    "        cv2.putText(frame, f'FPS: {int(fps)}', (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "        \n",
    "        cv2.imshow('Emotion Detection - Press Q to Quit', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('s'):\n",
    "            filename = f'screenshot_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.jpg'\n",
    "            cv2.imwrite(filename, frame)\n",
    "            print(f\"   Screenshot saved: {filename}\")\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"\\n✅ Session ended\")\n",
    "\n",
    "# Run detection\n",
    "realtime_emotion_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
